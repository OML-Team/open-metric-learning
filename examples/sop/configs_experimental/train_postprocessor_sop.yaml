postfix: "postprocessing"

seed: 42
precision: 16
accelerator: gpu
devices: 2

dataset_root: data/Stanford_Online_Products/
dataframe_name: df.csv
logs_root: logs/Stanford_Online_Products/
logs_folder: ${now:%Y-%m-%d_%H-%M-%S}_${postfix}

num_workers: 20
max_epochs: 1000
valid_period: 1

extractor:
  name: vit
  args:
    normalise_features: True
    use_multi_scale: False
    weights: vits16_sop
    arch: vits16

pairwise_model:
  name: concat_siamese
  args:
    mlp_hidden_dims: [384, 192]
    extractor: ${extractor}

hard_pairs_mining: True

optimizer:
  name: adam
  args:
    lr: 2e-3

freeze_n_epochs: 3
scheduling:
  scheduler_interval: epoch
  scheduler_frequency: 1
  scheduler:
    name: multi_step
    args:
      gamma: 5e-3
      milestones:
        - ${freeze_n_epochs}

sampler:
  name: category_balance
  args:
    n_labels: 6
    n_instances: 4
    n_categories: 5
    resample_labels: True
    weight_categories: True

bs_val: 128
postprocessing_top_n: 5
metric_for_checkpointing: OVERALL/cmc/1
log_images: False
metric_args:
  metrics_to_exclude_from_visualization: [cmc,]
  cmc_top_k: [1, 5]
  map_top_k: [5]
  fmr_vals: []  # Since SOP is a big dataset you should be careful with increasing of the memory footprint, which is needed to calculate fmr
  pfc_variance: []
  return_only_main_category: True
  visualize_only_main_category: True

neptune_project: null

transforms_train:
  name: augs_hypvit_torch
  args:
    im_size: 224
    min_scale: 0.8

transforms_val:
  name: norm_resize_hypvit_torch
  args:
    im_size: 224
    crop_size: 224

embeddings_cache_dir: ${dataset_root}
transforms_extraction:
  name: norm_resize_hypvit_torch
  args:
    im_size: 224
    crop_size: 224

tags:
  - sop
  - postprocessing

hydra_dir: ${logs_root}/${logs_folder}/

hydra:
  run:
    dir: ${hydra_dir}
  searchpath:
    - pkg://oml.configs
