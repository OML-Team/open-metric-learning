{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/ashabanov/code/metric_learning/open-metric-learning\")\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from oml.datasets.base import BaseDataset\n",
    "from oml.models.vit.vit import ViTExtractor\n",
    "from oml.transforms.images.torchvision.transforms import get_normalisation_resize_hypvit\n",
    "from oml.transforms.images.utils import get_im_reader_for_transforms\n",
    "from oml.metrics.embeddings import EmbeddingMetrics\n",
    "from oml.postprocessors.pairwise_embeddings import PairwiseEmbeddingsPostprocessor\n",
    "from oml.postprocessors.pairwise_images import PairwiseImagesPostprocessor\n",
    "from oml.samplers.balance import BalanceSampler\n",
    "from oml.samplers.category_balance import CategoryBalanceSampler\n",
    "from oml.miners.inbatch_all_tri import AllTripletsMiner\n",
    "from oml.miners.inbatch_hard_tri import HardTripletsMiner\n",
    "from oml.utils.misc_torch import elementwise_dist\n",
    "from torchvision.models import resnet50\n",
    "from oml.models.vit.vit import ViTExtractor\n",
    "from oml.transforms.images.torchvision.transforms import get_normalisation_resize_hypvit, get_augs_hypvit\n",
    "from oml.datasets.base import DatasetWithLabels\n",
    "from oml.transforms.images.utils import get_im_reader_for_transforms\n",
    "import torchvision.transforms as t\n",
    "from oml.const import MEAN, STD\n",
    "\n",
    "\n",
    "from source import TensorsWithLabels\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 330)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = Path(\"/nydl/data/DeepFashion_InShop/\")\n",
    "weights = \"vits16_inshop\"\n",
    "\n",
    "if False:  # save features\n",
    "    batch_size = 1024\n",
    "\n",
    "    df = pd.read_csv(dataset_root / \"df.csv\")\n",
    "\n",
    "    transform = get_normalisation_resize_hypvit(im_size=224, crop_size=224)\n",
    "    im_reader = get_im_reader_for_transforms(transform)\n",
    "\n",
    "    dataset = BaseDataset(df=df, transform=transform, f_imread=im_reader)\n",
    "    model = ViTExtractor(weights, arch=weights.split(\"_\")[0], normalise_features=True).eval().cuda()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=20)\n",
    "\n",
    "    embeddings = torch.zeros((len(df), model.feat_dim))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            embs = model(batch[\"input_tensors\"].cuda()).detach().cpu()\n",
    "            ia = i * batch_size\n",
    "            ib = min(len(embeddings), (i + 1) * batch_size)\n",
    "            embeddings[ia:ib, :] = embs\n",
    "\n",
    "    torch.save(embeddings, dataset_root / f\"embeddings_{weights}.pkl\")\n",
    "    \n",
    "    \n",
    "def get_datasets():\n",
    "    embeddings = torch.load(dataset_root / f\"embeddings_{weights}.pkl\")\n",
    "    df = pd.read_csv(dataset_root / \"df.csv\")\n",
    "    train_mask = df[\"split\"] == \"train\"\n",
    "    \n",
    "    emb_train = embeddings[train_mask]\n",
    "    emb_val = embeddings[~train_mask]\n",
    "    \n",
    "    df_train = df[train_mask]\n",
    "    df_train.reset_index(inplace=True)\n",
    "    \n",
    "    df_val = df[~train_mask]\n",
    "    df_val.reset_index(inplace=True)\n",
    "\n",
    "    return emb_train, emb_val, df_train, df_val\n",
    "\n",
    "\n",
    "emb_train, emb_val, df_train, df_val = get_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsMiner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.miner = HardTripletsMiner()\n",
    "#         self.miner = AllTripletsMiner()\n",
    "        \n",
    "    def sample(self, features, labels):\n",
    "        ii_a, ii_p, ii_n = self.miner._sample(features, labels=labels)\n",
    "            \n",
    "        ii_a_1, ii_p = zip(*list(set(list(map(lambda x: tuple(sorted([x[0], x[1]])), zip(ii_a, ii_p))))))\n",
    "        ii_a_2, ii_n = zip(*list(set(list(map(lambda x: tuple(sorted([x[0], x[1]])), zip(ii_a, ii_n))))))\n",
    "        \n",
    "        gt_distance = torch.ones(len(ii_a_1) + len(ii_a_2))\n",
    "        gt_distance[:len(ii_a_1)] = 0\n",
    "                                \n",
    "        return torch.tensor([*ii_a_1, *ii_a_2]).long(), torch.tensor([*ii_p, *ii_n]).long(), gt_distance\n",
    "    \n",
    "    \n",
    "miner = PairsMiner()\n",
    "miner.sample(torch.ones(10, 24), torch.tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesSiamese(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ImagesSiamese, self).__init__()\n",
    "        self.model = ViTExtractor(weights=weights, arch=\"vits16\", normalise_features=True)\n",
    "        feat_dim = self.model.feat_dim\n",
    "        \n",
    "        self.head = nn.Sequential(*[\n",
    "            nn.Linear(feat_dim, feat_dim // 2, bias=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(feat_dim // 2,  1, bias=False),\n",
    "        ])\n",
    "            \n",
    "        self.frozen = False\n",
    "        \n",
    "    def forward(self, x1, x2):   \n",
    "        x = torch.concat([x1, x2], dim=2)\n",
    "        \n",
    "        if self.frozen:\n",
    "            with torch.no_grad():\n",
    "                x = self.model(x)\n",
    "        else:\n",
    "            x = self.model(x)\n",
    "            \n",
    "        x = self.head(x)\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "ImagesSiamese()(torch.zeros(5, 3, 224, 224), torch.zeros(5, 3, 224, 224))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(pairwise_model):\n",
    "    if pairwise_model:\n",
    "        processor = PairwiseImagesPostprocessor(pairwise_model, top_n=5, image_transforms=get_normalisation_resize_hypvit(224, 224))\n",
    "    else:\n",
    "        processor = None\n",
    "\n",
    "    calculator = EmbeddingMetrics(\n",
    "        cmc_top_k=(1, 5),\n",
    "        postprocessor=processor,\n",
    "        extra_keys=(\"paths\",)\n",
    "    )\n",
    "    calculator.setup(len(df_val))\n",
    "    calculator.update_data({\n",
    "        \"embeddings\": emb_val,\n",
    "        \"is_query\": torch.tensor(df_val[\"is_query\"]).bool(),\n",
    "        \"is_gallery\": torch.tensor(df_val[\"is_gallery\"]).bool(),\n",
    "        \"labels\": torch.tensor(df_val[\"label\"]).long(),\n",
    "        \"paths\": df_val[\"path\"].tolist()\n",
    "    })\n",
    "    metrics = calculator.compute_metrics();\n",
    "    print(metrics)\n",
    "    \n",
    "    return calculator\n",
    "    \n",
    "    \n",
    "# calc = val(ImagesSiamese().cuda())\n",
    "# calc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-publicity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "model = ImagesSiamese().cuda().train()\n",
    "\n",
    "# transform = get_augs_hypvit()  # todo\n",
    "\n",
    "transform = t.Compose(\n",
    "        [\n",
    "            t.RandomResizedCrop((224, 224), scale=(0.8, 1.0), interpolation=t.InterpolationMode.BICUBIC),\n",
    "            t.RandomHorizontalFlip(),\n",
    "            t.ToTensor(),\n",
    "            t.Normalize(mean=MEAN, std=STD),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "f_imread = get_im_reader_for_transforms(transform)\n",
    "dataset = DatasetWithLabels(df_train, transform=transform, f_imread=f_imread, dataset_root=dataset_root)\n",
    "\n",
    "\n",
    "n_labels, n_instances, n_categories = 5, 4, 5\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    batch_sampler=CategoryBalanceSampler(labels=dataset.get_labels(),\n",
    "                                         label2category=dict(zip(df_train[\"label\"], df_train[\"category\"])),\n",
    "                                         n_labels=n_labels,\n",
    "                                         n_instances=n_instances, \n",
    "                                         n_categories=n_categories,\n",
    "                                         resample_labels=True,\n",
    "                                        ),\n",
    "    dataset=dataset, \n",
    "    num_workers=20\n",
    ")\n",
    "\n",
    "repeated = [next(iter(loader))] * 80\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-6)\n",
    "\n",
    "pairs_miner = PairsMiner()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "losses = []\n",
    "acc = []\n",
    "\n",
    "for i_epoch in range(1000):\n",
    "    tqdm_loader = tqdm(loader)\n",
    "    \n",
    "    if i_epoch < 1:\n",
    "        model.frozen = True\n",
    "        optimizer.param_groups[0]['lr'] = 1e-2\n",
    "    else:\n",
    "        model.frozen = False\n",
    "        optimizer.param_groups[0]['lr'] = 1e-6\n",
    "    \n",
    "    for batch in tqdm_loader:\n",
    "        features = emb_train[batch[\"idx\"]]\n",
    "        ii1, ii2, gt_dist = pairs_miner.sample(features, batch[\"labels\"])\n",
    "        x1, x2, gt_dist = batch[\"input_tensors\"][ii1].cuda(), batch[\"input_tensors\"][ii2].cuda(), gt_dist.cuda()\n",
    "\n",
    "        pred_dist = model(x1=x1, x2=x2)\n",
    "        loss = criterion(pred_dist, gt_dist)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # logs\n",
    "        loss_v = loss.detach().cpu().item()\n",
    "        accuracy = ((pred_dist > 0.5) == gt_dist).float().mean().item()\n",
    "        tqdm_loader.set_postfix({\"acc\": accuracy, \"loss\": loss_v})\n",
    "        losses.append(loss_v)\n",
    "        acc.append(accuracy)\n",
    "        \n",
    "        \n",
    "    if (i_epoch + 1) % 1 == 0:\n",
    "        val(model)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(acc)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_dist.detach().cpu().numpy(), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-camera",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-example",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-token",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-lawrence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-strain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-istanbul",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-publisher",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-nevada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-vietnam",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-closing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-knight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim: int, identity_init: bool = True):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.proj = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "\n",
    "        if identity_init:\n",
    "            ini_dict = {\"weight\": torch.eye(feat_dim), \"bias\": torch.zeros(feat_dim)}\n",
    "            self.proj.load_state_dict(ini_dict)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.proj(x1)\n",
    "        x2 = self.proj(x2)\n",
    "        \n",
    "        x1 = x1 / torch.linalg.norm(x1, 2, dim=1, keepdim=True).detach()\n",
    "        x2 = x2 / torch.linalg.norm(x2, 2, dim=1, keepdim=True).detach()\n",
    "        \n",
    "        x = (x1 * x2).sum(dim=1)\n",
    "        x = (1 - x) / 2\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Siamese2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim: int):\n",
    "        super(Siamese2, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "        self.fc  = torch.nn.Linear(4 * feat_dim, feat_dim, bias=True)\n",
    "        self.last = torch.nn.Linear(feat_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        y1 = torch.relu(self.fc1(x1 + x2))\n",
    "        y2 = torch.relu(self.fc2(x1 - x2))\n",
    "        y3 = torch.relu(self.fc2(x2 - x1))\n",
    "        y4 = torch.relu(self.fc3(x1 * x2))\n",
    "        \n",
    "        y = torch.relu(self.fc(torch.concat([y1, y2, y3, y4], dim=1)))\n",
    "        \n",
    "        y = torch.sigmoid(self.last(y)).squeeze()\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "y = Siamese2(3)(torch.tensor([[+1.0, 0, 0], [1, 0, 0]]), torch.tensor([[-1.0, 0, 0.0], [0, 1, 0]]))\n",
    "print(y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
