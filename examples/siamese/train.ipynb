{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "color-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/ashabanov/code/metric_learning/open-metric-learning\")\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from oml.datasets.base import BaseDataset\n",
    "from oml.models.vit.vit import ViTExtractor\n",
    "from oml.transforms.images.torchvision.transforms import get_normalisation_resize_hypvit\n",
    "from oml.transforms.images.utils import get_im_reader_for_transforms\n",
    "from oml.metrics.embeddings import EmbeddingMetrics\n",
    "from oml.postprocessors.pairwise_embeddings import PairwiseEmbeddingsPostprocessor\n",
    "from oml.postprocessors.pairwise_images import PairwiseImagesPostprocessor\n",
    "from oml.samplers.balance import BalanceSampler\n",
    "from oml.miners.inbatch_all_tri import AllTripletsMiner\n",
    "from oml.miners.inbatch_hard_tri import HardTripletsMiner\n",
    "from oml.utils.misc_torch import elementwise_dist\n",
    "from torchvision.models import resnet50\n",
    "from oml.models.vit.vit import ViTExtractor\n",
    "from oml.transforms.images.torchvision.transforms import get_normalisation_resize_hypvit\n",
    "from oml.datasets.base import DatasetWithLabels\n",
    "    \n",
    "from source import TensorsWithLabels\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 330)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suffering-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = Path(\"/nydl/data/DeepFashion_InShop/\")\n",
    "weights = \"vits16_dino\"\n",
    "\n",
    "if False:  # save features\n",
    "    batch_size = 1024\n",
    "\n",
    "    df = pd.read_csv(dataset_root / \"df.csv\")\n",
    "\n",
    "    transform = get_normalisation_resize_hypvit(im_size=224, crop_size=224)\n",
    "    im_reader = get_im_reader_for_transforms(transform)\n",
    "\n",
    "    dataset = BaseDataset(df=df, transform=transform, f_imread=im_reader)\n",
    "    model = ViTExtractor(weights, arch=weights.split(\"_\")[0], normalise_features=True).eval().cuda()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=20)\n",
    "\n",
    "    embeddings = torch.zeros((len(df), model.feat_dim))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            embs = model(batch[\"input_tensors\"].cuda()).detach().cpu()\n",
    "            ia = i * batch_size\n",
    "            ib = min(len(embeddings), (i + 1) * batch_size)\n",
    "            embeddings[ia:ib, :] = embs\n",
    "\n",
    "    torch.save(embeddings, dataset_root / f\"embeddings_{weights}.pkl\")\n",
    "    \n",
    "    \n",
    "def get_datasets():\n",
    "    embeddings = torch.load(dataset_root / f\"embeddings_{weights}.pkl\")\n",
    "    df = pd.read_csv(dataset_root / \"df.csv\")\n",
    "    train_mask = df[\"split\"] == \"train\"\n",
    "    \n",
    "    emb_train = embeddings[train_mask]\n",
    "    emb_val = embeddings[~train_mask]\n",
    "    \n",
    "    df_train = df[train_mask]\n",
    "    df_train.reset_index(inplace=True)\n",
    "    \n",
    "    df_val = df[~train_mask]\n",
    "    df_val.reset_index(inplace=True)\n",
    "\n",
    "    return emb_train, emb_val, df_train, df_val\n",
    "\n",
    "\n",
    "emb_train, emb_val, df_train, df_val = get_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsMiner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.miner = HardTripletsMiner()\n",
    "#         self.miner = AllTripletsMiner()\n",
    "        \n",
    "    def sample(self, features, labels):\n",
    "        ii_a, ii_p, ii_n = self.miner._sample(features, labels=labels)\n",
    "            \n",
    "        ii_a_1, ii_p = zip(*list(set(list(map(lambda x: tuple(sorted([x[0], x[1]])), zip(ii_a, ii_p))))))\n",
    "        ii_a_2, ii_n = zip(*list(set(list(map(lambda x: tuple(sorted([x[0], x[1]])), zip(ii_a, ii_n))))))\n",
    "        \n",
    "        gt_distance = torch.ones(len(ii_a_1) + len(ii_a_2))\n",
    "        gt_distance[:len(ii_a_1)] = 0\n",
    "                                \n",
    "        return features[[*ii_a_1, *ii_a_2]], features[[*ii_p, *ii_n]], gt_distance\n",
    "    \n",
    "    \n",
    "miner = PairsMiner()\n",
    "miner.sample(torch.ones(10, 24), torch.tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim: int, identity_init: bool = True):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.proj = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "\n",
    "        if identity_init:\n",
    "            ini_dict = {\"weight\": torch.eye(feat_dim), \"bias\": torch.zeros(feat_dim)}\n",
    "            self.proj.load_state_dict(ini_dict)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.proj(x1)\n",
    "        x2 = self.proj(x2)\n",
    "        \n",
    "        x1 = x1 / torch.linalg.norm(x1, 2, dim=1, keepdim=True).detach()\n",
    "        x2 = x2 / torch.linalg.norm(x2, 2, dim=1, keepdim=True).detach()\n",
    "        \n",
    "        x = (x1 * x2).sum(dim=1)\n",
    "        x = (1 - x) / 2\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Siamese2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim: int):\n",
    "        super(Siamese2, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(feat_dim, feat_dim, bias=True)\n",
    "        self.fc  = torch.nn.Linear(4 * feat_dim, feat_dim, bias=True)\n",
    "        self.last = torch.nn.Linear(feat_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        y1 = torch.relu(self.fc1(x1 + x2))\n",
    "        y2 = torch.relu(self.fc2(x1 - x2))\n",
    "        y3 = torch.relu(self.fc2(x2 - x1))\n",
    "        y4 = torch.relu(self.fc3(x1 * x2))\n",
    "        \n",
    "        y = torch.relu(self.fc(torch.concat([y1, y2, y3, y4], dim=1)))\n",
    "        \n",
    "        y = torch.sigmoid(self.last(y)).squeeze()\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "y = Siamese2(3)(torch.tensor([[+1.0, 0, 0], [1, 0, 0]]), torch.tensor([[-1.0, 0, 0.0], [0, 1, 0]]))\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "royal-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesSiamese(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ImagesSiamese, self).__init__()\n",
    "        self.model = ViTExtractor(weights=weights, arch=\"vits16\", normalise_features=True)\n",
    "        self.fc = torch.nn.Linear(in_features=self.model.feat_dim * 2, out_features=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.model(x1)\n",
    "        x2 = self.model(x2)\n",
    "        \n",
    "        x1 = x1 / torch.linalg.norm(x1, 2, dim=1, keepdim=True).detach()\n",
    "        x2 = x2 / torch.linalg.norm(x2, 2, dim=1, keepdim=True).detach()\n",
    "        \n",
    "        x = (x1 * x2).sum(dim=1)\n",
    "        x = (1 - x) / 2\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "composite-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\n",
      "Checkpoint is already here.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998aa51768334e41ba19137d410f2953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/556 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:\n",
      "{'OVERALL': {'cmc': {1: tensor(0.4602), 5: tensor(0.6329)},\n",
      "             'map': {5: tensor(0.2762)},\n",
      "             'pcf': {0.5: tensor(0.0339)},\n",
      "             'precision': {5: tensor(0.3315)}}}\n",
      "{'OVERALL': {'cmc': {1: tensor(0.4602), 5: tensor(0.6329)}, 'precision': {5: tensor(0.3315)}, 'map': {5: tensor(0.2762)}, 'pcf': {0.5: tensor(0.0339)}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<oml.metrics.embeddings.EmbeddingMetrics at 0x7f6aac8bed30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def val(pairwise_model):\n",
    "    if pairwise_model:\n",
    "        processor = PairwiseImagesPostprocessor(pairwise_model, top_n=5, image_transforms=get_normalisation_resize_hypvit(224, 224))\n",
    "#         processor = PairwiseEmbeddingsPostprocessor(pairwise_model, top_n=5)\n",
    "    else:\n",
    "        processor = None\n",
    "\n",
    "    calculator = EmbeddingMetrics(\n",
    "        cmc_top_k=(1, 5),\n",
    "        postprocessor=processor,\n",
    "        extra_keys=(\"paths\",)\n",
    "    )\n",
    "    calculator.setup(len(df_val))\n",
    "    calculator.update_data({\n",
    "        \"embeddings\": emb_val,\n",
    "        \"is_query\": torch.tensor(df_val[\"is_query\"]).bool(),\n",
    "        \"is_gallery\": torch.tensor(df_val[\"is_gallery\"]).bool(),\n",
    "        \"labels\": torch.tensor(df_val[\"label\"]).long(),\n",
    "        \"paths\": df_val[\"path\"].tolist()\n",
    "    })\n",
    "    metrics = calculator.compute_metrics();\n",
    "    print(metrics)\n",
    "    \n",
    "    return calculator\n",
    "    \n",
    "    \n",
    "calc = val(ImagesSiamese().cuda())\n",
    "calc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-publicity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "model = ImagesSiamese().cuda()\n",
    "model.cuda().train()\n",
    "\n",
    "dataset = TensorsWithLabels(df_train, emb_train)\n",
    "\n",
    "n_labels, n_instances = 100, 4\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    batch_sampler=BalanceSampler(labels=dataset.get_labels(), n_labels=n_labels, n_instances=n_instances),\n",
    "    dataset=dataset, num_workers=0\n",
    ")\n",
    "\n",
    "repeated = [next(iter(loader))] * 50\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "pairs_miner = PairsMiner()\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "\n",
    "losses = []\n",
    "acc = []\n",
    "\n",
    "for i_epoch in range(20):\n",
    "    tqdm_loader = tqdm(repeated)\n",
    "    for batch in tqdm_loader:\n",
    "        x1, x2, gt_dist = pairs_miner.sample(batch[\"input_tensors\"], batch[\"labels\"])\n",
    "        x1, x2, gt_dist = x1.cuda(), x2.cuda(), gt_dist.cuda()\n",
    "\n",
    "        pred_dist = model(x1=x1, x2=x2)\n",
    "        loss = criterion(pred_dist, gt_dist)\n",
    "                        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # logs\n",
    "        accuracy = ((pred_dist > 0.025) == gt_dist).float().mean().item()\n",
    "        tqdm_loader.set_postfix({\"acc\": accuracy, \"loss\": loss.item()})\n",
    "        losses.append(loss.item())\n",
    "        acc.append(accuracy)\n",
    "        \n",
    "        \n",
    "    if i_epoch % 1 == 0:\n",
    "        val(model)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_dist.detach().cpu().numpy(), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-liberty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
